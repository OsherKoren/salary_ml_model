{"cells":[{"cell_type":"code","source":["%run ../ml"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"21addbe2-fcc6-4939-8181-ceb68a7879d5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[563]: {'EdLevel': {'Primary/elementary school': 1.0,\n  'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)': 2.0,\n  'Associate degree (A.A., A.S., etc.)': 3.0,\n  'Some college/university study without earning a degree': 4.0,\n  'Something else, Professional degree (JD, MD, etc.)': 5.0,\n  'Bachelor’s degree (B.A., B.S., B.Eng., etc.)': 6.0,\n  'Master’s degree (M.A., M.S., M.Eng., MBA, etc.)': 7.0,\n  'Other doctoral degree (Ph.D., Ed.D., etc.)': 8.0},\n 'Age1stCode': {'Younger than 5 years': 1.0,\n  '5 - 10 years': 2.0,\n  '11 - 17 years': 3.0,\n  '18 - 24 years': 4.0,\n  '25 - 34 years': 5.0,\n  '35 - 44 years': 6.0,\n  '45 - 54 years': 7.0,\n  '55 - 64 years': 8.0,\n  'Older than 64 years': 9.0},\n 'OrgSize': {'Just me - I am a freelancer, sole proprietor, etc.': 1.0,\n  '2 to 9 employees': 2.0,\n  '10 to 19 employees': 3.0,\n  '20 to 99 employees': 4.0,\n  '100 to 499 employees': 5.0,\n  'I don’t know': 6.0,\n  '500 to 999 employees': 7.0,\n  '1,000 to 4,999 employees': 8.0,\n  '5,000 to 9,999 employees': 9.0,\n  '10,000 or more employees': 10.0},\n 'Age': {'Under 18 years old': 1.0,\n  '18-24 years old': 2.0,\n  '25-34 years old': 3.0,\n  '35-44 years old': 4.0,\n  '45-54 years old': 5.0,\n  '55-64 years old': 6.0,\n  '65 years or older': 7.0,\n  'Prefer not to say': 8.0}}"]}],"execution_count":0},{"cell_type":"code","source":["RUN_XGB_EXPERIMENT: bool = True\nRUN_RF_EXPERIMENT: bool = True"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f698462a-22ff-42e6-b839-faa7b4cd1d48","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = _set_df(cols_to_drop=[\"ResponseId\"], selected_features=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4ccc7fce-d3d4-4af7-ab76-731212320268","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(f'# of Columns: {len(df.columns)}')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bb5137b6-857b-48c6-83a2-72dd25127527","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["# of Columns: 43\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Experiments models"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"547352f2-f0ee-45cf-b6b1-2d975937d100","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##### RandomForestRegressor with default parameters"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dfb02a06-b578-4b43-9ccf-ac13d1269c5c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["if RUN_RF_EXPERIMENT:\n    rf_model_name = \"rf_reg\"\n    rf_reg = RandomForestRegressor(labelCol=TRANSFORMED_TARGET, seed=SEED)\n    rf_param_grid = _set_rf_reg_param_grid(estimator=rf_reg)\n    cv_rf_reg = tune_param(estimator=rf_reg, param_grid=rf_param_grid)\n\n    rf_model = Experiment(df=df, model_name=rf_model_name, predictor=cv_rf_reg)\n    rf_model.run_mlflow()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c3ed2a51-675a-45ba-927f-a3da1abc2c03","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stderr","text":["2023/05/13 13:10:26 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:10:26 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2023/05/13 13:10:26 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/05/13 13:10:26 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n2023/05/13 13:10:27 WARNING mlflow.utils: Truncated the value of the key `VectorAssembler.inputCols`. Truncated value: `['CountryIndexEncoded', 'EthnicityIndexEncoded', 'LearnCodeIndexEncoded', 'OpSysIndexEncoded', 'SOVisitFreqIndexEncoded', 'SOCommIndexEncoded', 'EmploymentIndexEncoded', 'NEWSOSitesIndexEncoded', 'ToolsTechHaveWorkedWith_Terraform', 'DatabaseWantToWorkWith_MongoDB', 'LanguageHaveWorkedWith_PHP', 'LanguageHaveWorkedWith_Bash/Shell', 'DatabaseHaveWorkedWith_MySQL', 'PlatformHaveWorkedWith_AWS', 'ToolsTechHaveWorkedWith_Kubernetes', 'DevType_Engineering manager', 'NEWCollabToolsHaveWorkedWith_An...`\n2023/05/13 13:12:30 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:12:30 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:12:30 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:12:30 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:12:33 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:12:33 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:12:33 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:12:33 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:12:35 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:12:35 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:12:35 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:12:35 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:13:34 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2023/05/13 13:15:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:15:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2023/05/13 13:15:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/05/13 13:15:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n"]},{"output_type":"stream","output_type":"stream","name":"stdout","text":["\u001B[36mTrain set size: 30244\n\u001B[36mTest set size: 7404\n\nyeojohnson lambda: 0.255378410184943\n ====================================================================================================\nrf_reg - train - evaluation metrics:\n ====================================================================================================\nRMSE on train set:  8.9\nThe mean value on the train set:  59\nRMSE mean ratio on train set:  0.1492\n ====================================================================================================\nrf_reg - test - evaluation metrics:\n ====================================================================================================\nRMSE on test set:  8.9\nThe mean value on the test set:  60\nRMSE mean ratio on test set:  0.1493\n ====================================================================================================\nThe table: train_rf_reg was saved.   File path: s3a://so-salary-survey/gold/train_rf_reg.parquet\nThe table: test_rf_reg was saved.   File path: s3a://so-salary-survey/gold/test_rf_reg.parquet\n"]}],"execution_count":0},{"cell_type":"markdown","source":["##### XgboostRegressor with grid search for hyperparameters tuning and cross validation"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a98e7396-7bb0-41d8-b924-5b8dd5446992","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# https://people.stat.sc.edu/haigang/improvement.html"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e53c2c66-ab62-4eb3-9e6a-b3a572cadbc4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["if RUN_XGB_EXPERIMENT:\n    xgb_model_name = \"xgb_reg\"\n    xgb_reg = XgboostRegressor(\n        labelCol=TRANSFORMED_TARGET, num_workers=3, missing=0.0, seed=SEED\n    )\n    # xgb_param_grid = _set_xgb_reg_param_grid(estimator=xgb_reg)  # Default parameters\n    xgb_param_grid = _set_xgb_reg_param_grid(\n        estimator=xgb_reg,\n        learning_rate=[0.01, 0.005, 0.1, 0.15],\n        max_depth=[2, 3],\n        gamma=[0, 0.005, 0.01],\n    )\n    #     xgb_param_grid = _set_xgb_reg_param_grid(estimator=xgb_reg, learning_rate=[0.01, 0.1], max_depth=[2, 3])\n    cv_xgb_reg = tune_param(estimator=xgb_reg, param_grid=xgb_param_grid)\n\n    xgb_model = Experiment(df=df, model_name=xgb_model_name, predictor=cv_xgb_reg)\n    xgb_model.run_mlflow()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f41818f8-bfd7-4177-866b-2cdca3c833c4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stderr","text":["2023/05/13 13:17:04 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:04 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:17:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2023/05/13 13:17:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/05/13 13:17:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n2023/05/13 13:17:05 WARNING mlflow.utils: Truncated the value of the key `VectorAssembler.inputCols`. Truncated value: `['CountryIndexEncoded', 'EthnicityIndexEncoded', 'LearnCodeIndexEncoded', 'OpSysIndexEncoded', 'SOVisitFreqIndexEncoded', 'SOCommIndexEncoded', 'EmploymentIndexEncoded', 'NEWSOSitesIndexEncoded', 'ToolsTechHaveWorkedWith_Terraform', 'DatabaseWantToWorkWith_MongoDB', 'LanguageHaveWorkedWith_PHP', 'LanguageHaveWorkedWith_Bash/Shell', 'DatabaseHaveWorkedWith_MySQL', 'PlatformHaveWorkedWith_AWS', 'ToolsTechHaveWorkedWith_Kubernetes', 'DevType_Engineering manager', 'NEWCollabToolsHaveWorkedWith_An...`\n2023/05/13 13:27:57 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:27:57 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:27:57 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:27:57 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:27:59 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:27:59 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:27:59 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:27:59 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:28:01 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:28:01 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:28:01 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_original_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:28:01 DEBUG mlflow.utils.autologging_utils: Failed to log autologging event via '<bound method DatabricksAutologgingEventLogger.log_patch_function_success of <DatabricksAutologgingEventLogger.DatabricksAutologgingEventLogger object at 0x7f29107520d0>>'. Exception: 'NoneType' object has no attribute 'session'\n2023/05/13 13:32:13 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2023/05/13 13:35:43 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/982942755585532/114d2824bb63443299cda7e86c26f346/artifacts/xgb_reg/sparkml, flavor: spark), fall back to return ['pyspark==3.3.2']. Set logging level to DEBUG to see the full traceback.\n2023/05/13 13:35:43 DEBUG mlflow.utils.environment: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/mlflow/utils/environment.py\", line 391, in infer_pip_requirements\n    return _infer_requirements(model_uri, flavor)\n  File \"/databricks/python/lib/python3.9/site-packages/mlflow/utils/requirements_utils.py\", line 381, in _infer_requirements\n    modules = _capture_imported_modules(model_uri, flavor)\n  File \"/databricks/python/lib/python3.9/site-packages/mlflow/utils/requirements_utils.py\", line 284, in _capture_imported_modules\n    _run_command(\n  File \"/databricks/python/lib/python3.9/site-packages/mlflow/utils/requirements_utils.py\", line 223, in _run_command\n    raise MlflowException(msg)\nmlflow.exceptions.MlflowException: Encountered an unexpected error while running ['/local_disk0/.ephemeral_nfs/envs/pythonEnv-3518947d-994e-4c8e-a28c-887e5cfb0e95/bin/python', '/databricks/python/lib/python3.9/site-packages/mlflow/utils/_capture_modules.py', '--model-path', '/tmp/tmphnhs5imc/sparkml', '--flavor', 'spark', '--output-file', '/tmp/tmp42ioushv/imported_modules.txt', '--sys-path', '[\"/databricks/python_shell/scripts\", \"/databricks/python/lib/python3.9/site-packages/git/ext/gitdb\", \"/local_disk0/spark-9ed38cd7-8a0d-4ca6-a61f-03401f16db3f/userFiles-f562d59a-fceb-4118-98c8-643c18932fb2\", \"/databricks/spark/python\", \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip\", \"/databricks/jars/spark--driver--driver-spark_3.3_2.12_deploy.jar\", \"/WSFS_NOTEBOOK_DIR\", \"/databricks/jars/spark--maven-trees--ml--12.x--graphframes--org.graphframes--graphframes_2.12--org.graphframes__graphframes_2.12__0.8.2-db1-spark3.2.jar\", \"/databricks/python_shell\", \"/usr/lib/python39.zip\", \"/usr/lib/python3.9\", \"/usr/lib/python3.9/lib-dynload\", \"\", \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-3518947d-994e-4c8e-a28c-887e5cfb0e95/lib/python3.9/site-packages\", \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages\", \"/databricks/python/lib/python3.9/site-packages\", \"/usr/local/lib/python3.9/dist-packages\", \"/usr/lib/python3/dist-packages\", \"/databricks/.python_edge_libs\"]']\nexit status: -9\nstdout: 23/05/13 13:34:05 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.\n23/05/13 13:34:08 ERROR SparkConnectService: Could not start Spark Connect GRPC service\njava.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002\n\tat grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)\n\tat grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)\n\tat grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)\n\tat org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:286)\n\tat org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:292)\n\tat org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)\n\tat org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n\tat org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)\n\tat org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:234)\n\tat org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:736)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:257)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use\n23/05/13 13:34:52 WARN CloudStoreSpecificConf: Unknown cloud store file\n23/05/13 13:35:31 WARN CloudStoreSpecificConf: Unknown cloud store file\n23/05/13 13:35:32 WARN CloudStoreSpecificConf: Unknown cloud store file\n23/05/13 13:35:33 WARN CloudStoreSpecificConf: Unknown cloud store file\n23/05/13 13:35:34 WARN CloudStoreSpecificConf: Unknown cloud store file\n23/05/13 13:35:35 WARN CloudStoreSpecificConf: Unknown cloud store file\n23/05/13 13:35:36 WARN CloudStoreSpecificConf: Unknown cloud store file\n23/05/13 13:35:37 WARN CloudStoreSpecificConf: Unknown cloud store file\n23/05/13 13:35:38 WARN CloudStoreSpecificConf: Unknown cloud store file\n23/05/13 13:35:39 WARN CloudStoreSpecificConf: Unknown cloud store file\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR InstanceSpecificConf: Cannot access SparkConf in InstanceSpecificConf.isSqlGatewayEnabled().\n23/05/13 13:35:40 ERROR SQLUsageLogging: Exception while logging query profile\njava.lang.Exception: Unable to get a log redactor from SparkEnv = null\n\tat com.databricks.spark.util.DatabricksSparkLogRedactor$.redact(LogRedactor.scala:77)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:101)\n\tat com.databricks.sql.serialization.marshalling.AttributeReferenceMarshaller$.buildProto(leafExpressionMarshallers.scala:46)\n\tat com.databricks.sql.serialization.marshalling.AttributeReferenceMarshaller$.buildProto(leafExpressionMarshallers.scala:38)\n\tat com.databricks.sql.serialization.marshalling.SingleTagExpressionMarshaller.toProto(ExpressionMarshaller.scala:72)\n\tat com.databricks.sql.serialization.marshalling.SingleTagExpressionMarshaller.toProto$(ExpressionMarshaller.scala:62)\n\tat com.databricks.sql.serialization.marshalling.SingleExpressionTypeMarshaller.toProto(ExpressionMarshaller.scala:236)\n\tat com.databricks.sql.serialization.marshalling.SingleExpressionTypeMarshaller.toProto(ExpressionMarshaller.scala:236)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.ExpressionMarshaller$.toProto(ExpressionMarshaller.scala:47)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:96)\n\tat com.databricks.sql.serialization.marshalling.ExpressionChildWriter.$anonfun$writeExpressions$1(childUtils.scala:313)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.sql.serialization.marshalling.ExpressionChildWriter.writeExpressions(childUtils.scala:313)\n\tat com.databricks.sql.serialization.marshalling.FallbackDataSourceScanExecMarshaller$.buildUnknownDataSourceScanExecProto(FallbackDataSourceScanExecMarshaller.scala:114)\n\tat com.databricks.sql.serialization.marshalling.FallbackDataSourceScanExecMarshaller$.tryToProto(FallbackDataSourceScanExecMarshaller.scala:61)\n\tat com.databricks.sql.serialization.marshalling.FallbackDataSourceScanExecMarshaller$.tryToProto(FallbackDataSourceScanExecMarshaller.scala:47)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.$anonfun$tryToProto$1(interfaces.scala:154)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.InputAdapterMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:779)\n\tat com.databricks.sql.serialization.marshalling.InputAdapterMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:771)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.ColumnarToRowExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:195)\n\tat com.databricks.sql.serialization.marshalling.ColumnarToRowExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:189)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.WholeStageCodegenExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:753)\n\tat com.databricks.sql.serialization.marshalling.WholeStageCodegenExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:745)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.LimitExecMarshaller.buildProto(singleSparkPlanTypeMarshallers.scala:968)\n\tat com.databricks.sql.serialization.marshalling.LimitExecMarshaller.buildProto$(singleSparkPlanTypeMarshallers.scala:967)\n\tat com.databricks.sql.serialization.marshalling.CollectLimitExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:1008)\n\tat com.databricks.sql.serialization.marshalling.CollectLimitExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:1008)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.mars\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nnChildWriter.writeExpressions(childUtils.scala:313)\n\tat com.databricks.sql.serialization.marshalling.FallbackDataSourceScanExecMarshaller$.buildUnknownDataSourceScanExecProto(FallbackDataSourceScanExecMarshaller.scala:114)\n\tat com.databricks.sql.serialization.marshalling.FallbackDataSourceScanExecMarshaller$.tryToProto(FallbackDataSourceScanExecMarshaller.scala:61)\n\tat com.databricks.sql.serialization.marshalling.FallbackDataSourceScanExecMarshaller$.tryToProto(FallbackDataSourceScanExecMarshaller.scala:47)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.$anonfun$tryToProto$1(interfaces.scala:154)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.InputAdapterMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:779)\n\tat com.databricks.sql.serialization.marshalling.InputAdapterMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:771)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.ColumnarToRowExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:195)\n\tat com.databricks.sql.serialization.marshalling.ColumnarToRowExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:189)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.WholeStageCodegenExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:753)\n\tat com.databricks.sql.serialization.marshalling.WholeStageCodegenExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:745)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.LimitExecMarshaller.buildProto(singleSparkPlanTypeMarshallers.scala:968)\n\tat com.databricks.sql.serialization.marshalling.LimitExecMarshaller.buildProto$(singleSparkPlanTypeMarshallers.scala:967)\n\tat com.databricks.sql.serialization.marshalling.CollectLimitExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:1008)\n\tat com.databricks.sql.serialization.marshalling.CollectLimitExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:1008)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.logging.QueryProfile$.toProto(QueryProfile.scala:506)\n\tat com.databricks.sql.logging.QueryProfile$.toProtoBase64(QueryProfile.scala:634)\n\tat com.databricks.sql.logging.SQLUsageLogging$.logQueryProfile(SQLUsageLogging.scala:251)\n\tat com.databricks.sql.logging.SQLUsageLogging$.tryLogQueryProfile(SQLUsageLogging.scala:301)\n\tat com.databricks.sql.logging.SQLUsageLogging$.$anonfun$tryLogQueryProfileWrapper$1(SQLUsageLogging.scala:384)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n23/05/13 13:35:40 ERROR SQLUsageLogging: Exception while logging query profile\njava.lang.Exception: Unable to get a log redactor from SparkEnv = null\n\tat com.databricks.spark.util.DatabricksSparkLogRedactor$.redact(LogRedactor.scala:77)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:101)\n\tat com.databricks.sql.serialization.marshalling.AttributeReferenceMarshaller$.buildProto(leafExpressionMarshallers.scala:46)\n\tat com.databricks.sql.serialization.marshalling.AttributeReferenceMarshaller$.buildProto(leafExpressionMarshallers.scala:38)\n\tat com.databricks.sql.serialization.marshalling.SingleTagExpressionMarshaller.toProto(ExpressionMarshaller.scala:72)\n\tat com.databricks.sql.serialization.marshalling.SingleTagExpressionMarshaller.toProto$(ExpressionMarshaller.scala:62)\n\tat com.databricks.sql.serialization.marshalling.SingleExpressionTypeMarshaller.toProto(ExpressionMarshaller.scala:236)\n\tat com.databricks.sql.serialization.marshalling.SingleExpressionTypeMarshaller.toProto(ExpressionMarshaller.scala:236)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.ExpressionMarshaller$.toProto(ExpressionMarshaller.scala:47)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:96)\n\tat com.databricks.sql.serialization.marshalling.ExpressionChildWriter.$anonfun$writeExpressions$1(childUtils.scala:313)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.sql.serialization.marshalling.ExpressionChildWriter.writeExpressions(childUtils.scala:313)\n\tat com.databricks.sql.serialization.marshalling.FallbackDataSourceScanExecMarshaller$.buildUnknownDataSourceScanExecProto(FallbackDataSourceScanExecMarshaller.scala:114)\n\tat com.databricks.sql.serialization.marshalling.FallbackDataSourceScanExecMarshaller$.tryToProto(FallbackDataSourceScanExecMarshaller.scala:61)\n\tat com.databricks.sql.serialization.marshalling.FallbackDataSourceScanExecMarshaller$.tryToProto(FallbackDataSourceScanExecMarshaller.scala:47)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.$anonfun$tryToProto$1(interfaces.scala:154)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.InputAdapterMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:779)\n\tat com.databricks.sql.serialization.marshalling.InputAdapterMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:771)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.ColumnarToRowExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:195)\n\tat com.databricks.sql.serialization.marshalling.ColumnarToRowExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:189)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.WholeStageCodegenExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:753)\n\tat com.databricks.sql.serialization.marshalling.WholeStageCodegenExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:745)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanChildWriter.writePlan(childUtils.scala:339)\n\tat com.databricks.sql.serialization.marshalling.LimitExecMarshaller.buildProto(singleSparkPlanTypeMarshallers.scala:968)\n\tat com.databricks.sql.serialization.marshalling.LimitExecMarshaller.buildProto$(singleSparkPlanTypeMarshallers.scala:967)\n\tat com.databricks.sql.serialization.marshalling.CollectLimitExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:1008)\n\tat com.databricks.sql.serialization.marshalling.CollectLimitExecMarshaller$.buildProto(singleSparkPlanTypeMarshallers.scala:1008)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:578)\n\tat com.databricks.sql.serialization.marshalling.SingleSparkPlanTypeMarshaller.toProto(SparkPlanMarshaller.scala:570)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.$anonfun$tryToProto$1(singleTypeMarshallers.scala:100)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.sql.serialization.marshalling.SingleTypeMarshallerRegistry.tryToProto(singleTypeMarshallers.scala:100)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$1.tryToProto(interfaces.scala:154)\n\tat com.databricks.sql.serialization.marshalling.PartialMarshaller$$anon$2.toProto(interfaces.scala:173)\n\tat com.databricks.sql.serialization.marshalling.SparkPlanMarshaller$.toProto(SparkPlanMarshaller.scala:53)\n\tat com.databricks.sql.serialization.marshalling.MarshallingContextImpl.toProto(interfaces.scala:94)\n\tat com.databricks.sql.logging.QueryProfile$.toProto(QueryProfile.scala:506)\n\tat com.databricks.sql.logging.QueryProfile$.toProtoBase64(QueryProfile.scala:634)\n\tat com.databricks.sql.logging.SQLUsageLogging$.logQueryProfile(SQLUsageLogging.scala:251)\n\tat com.databricks.sql.logging.SQLUsageLogging$.tryLogQueryProfile(SQLUsageLogging.scala:301)\n\tat com.databricks.sql.logging.SQLUsageLogging$.$anonfun$tryLogQueryProfileWrapper$1(SQLUsageLogging.scala:384)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nstderr: ERROR StatusLogger Reconfiguration failed: No configuration found for '2669b199' at 'null' in 'null'\nERROR StatusLogger Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n2023/05/13 13:34:26 INFO mlflow.spark: File '/tmp/tmphnhs5imc/sparkml' is already on DFS, copy is not necessary.\n\r[Stage 0:>                                                          (0 + 0) / 1]\r\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 3:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 4:>                                                          (0 + 1) / 1]\r\r                                                                                \r\n2023/05/13 13:35:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 DEBUG mlflow.utils.gorilla: Patch fn on destination already existed. Overwrite old patch.\n2023/05/13 13:35:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2023/05/13 13:35:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/05/13 13:35:44 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n"]},{"output_type":"stream","output_type":"stream","name":"stdout","text":["\u001B[36mTrain set size: 30244\n\u001B[36mTest set size: 7404\n\nyeojohnson lambda: 0.255378410184943\n ====================================================================================================\nxgb_reg - train - evaluation metrics:\n ====================================================================================================\nRMSE on train set:  7.4\nThe mean value on the train set:  59\nRMSE mean ratio on train set:  0.1252\n ====================================================================================================\nxgb_reg - test - evaluation metrics:\n ====================================================================================================\nRMSE on test set:  7.5\nThe mean value on the test set:  60\nRMSE mean ratio on test set:  0.1263\n ====================================================================================================\nThe table: train_xgb_reg was saved.   File path: s3a://so-salary-survey/gold/train_xgb_reg.parquet\nThe table: test_xgb_reg was saved.   File path: s3a://so-salary-survey/gold/test_xgb_reg.parquet\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"24ce6611-de75-4086-9e3e-ffaf80b3b9b4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"run_ml","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
