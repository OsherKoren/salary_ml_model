{"cells":[{"cell_type":"code","source":["%run ./feature_engineering"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1eb55a47-3118-4f6f-ae87-eec0012f1696","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[31]: {'EdLevel': {'Primary/elementary school': 1.0,\n  'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)': 2.0,\n  'Associate degree (A.A., A.S., etc.)': 3.0,\n  'Some college/university study without earning a degree': 4.0,\n  'Something else, Professional degree (JD, MD, etc.)': 5.0,\n  'Bachelor’s degree (B.A., B.S., B.Eng., etc.)': 6.0,\n  'Master’s degree (M.A., M.S., M.Eng., MBA, etc.)': 7.0,\n  'Other doctoral degree (Ph.D., Ed.D., etc.)': 8.0},\n 'Age1stCode': {'Younger than 5 years': 1.0,\n  '5 - 10 years': 2.0,\n  '11 - 17 years': 3.0,\n  '18 - 24 years': 4.0,\n  '25 - 34 years': 5.0,\n  '35 - 44 years': 6.0,\n  '45 - 54 years': 7.0,\n  '55 - 64 years': 8.0,\n  'Older than 64 years': 9.0},\n 'OrgSize': {'Just me - I am a freelancer, sole proprietor, etc.': 1.0,\n  '2 to 9 employees': 2.0,\n  '10 to 19 employees': 3.0,\n  '20 to 99 employees': 4.0,\n  '100 to 499 employees': 5.0,\n  'I don’t know': 6.0,\n  '500 to 999 employees': 7.0,\n  '1,000 to 4,999 employees': 8.0,\n  '5,000 to 9,999 employees': 9.0,\n  '10,000 or more employees': 10.0},\n 'Age': {'Under 18 years old': 1.0,\n  '18-24 years old': 2.0,\n  '25-34 years old': 3.0,\n  '35-44 years old': 4.0,\n  '45-54 years old': 5.0,\n  '55-64 years old': 6.0,\n  '65 years or older': 7.0,\n  'Prefer not to say': 8.0}}"]},{"output_type":"stream","output_type":"stream","name":"stderr","text":["/databricks/python_shell/dbruntime/PostImportHook.py:184: FutureWarning: `sparkdl.xgboost` is deprecated and will be removed in a future Databricks Runtime release. Use `xgboost.spark` instead. See https://docs.databricks.com/machine-learning/train-model/xgboost-spark.html#xgboost-migration for migration.\n  hook(module)\n"]}],"execution_count":0},{"cell_type":"code","source":["from dataclasses import dataclass, field\nimport mlflow\nimport mlflow.spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"311a723d-dfcd-4c13-8b8a-2e8e70dab6de","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["TRANSFORMED_TARGET = f\"transformed_{TARGET_COL}\"\nSEED: int = 42"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"652d26aa-b3dd-4a43-8eb2-6325bd6b7468","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/\ndef calc_metrics(\n    *,\n    df: DataFrame,\n    model_name: str,\n    ds_name: str = \"test\",\n    target_col: str = TRANSFORMED_TARGET,\n    pred_col: str = \"prediction\",\n) -> Tuple[float, float, float]:\n    \"\"\"The function gets the dataframe of the prediction and the model name, and prints the evaluation metrics (RMSE, RMSE vs mean)\n    :param df: A pyspark.sql.dataframe.DataFrame object.\n    :parma model_name: The model name as identifier.\n    :parma ds_name: The dataset name as identifier: train or test.\n    :parma target_col: The target column's name.\n    :return: Tuple[float, float, float]\n    \"\"\"\n    evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=pred_col)\n    _rmse = evaluator.evaluate(df)\n    print(f\"{model_name} - {ds_name} - evaluation metrics:\\n\", \"=\" * 100)\n    print(f\"RMSE on {ds_name} set: {_rmse: .1f}\")\n    _mean = df.select(f.mean(target_col)).collect()[0][0]\n    print(f\"The mean value on the {ds_name} set: {_mean: .0f}\")\n    rmse_mean_ratio = _rmse / _mean\n    print(f\"RMSE mean ratio on {ds_name} set: {rmse_mean_ratio: .4f}\\n\", \"=\" * 100)\n    return _rmse, _mean, rmse_mean_ratio"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d03e3b0e-3708-44b2-a06b-338d10b9772b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@dataclass\nclass Experiment:\n    \"\"\"A class for running ML experiments\n    Attributes:\n    df: A pyspark.sql.dataframe.DataFrame object. The dataset of the total data.\n    model_name: str. The name as identifier of the model.\n    predictor: ModelRegressor. The predictor model.\n    test_fraction: float. The fraction of the test dataset from total data. Set to default 0.2.\n    seed: int. A seed number for consistency.\n    target_col: str. The target variable to predict. Set to default \"ConvertedCompYearly\".\n    train: A pyspark.sql.dataframe.DataFrame object. The train dataset of the data.\n    test: A pyspark.sql.dataframe.DataFrame object. The test dataset of the data.\n    \"\"\"\n\n    df: DataFrame\n    model_name: str\n    predictor: ModelRegressor\n    target_col: str = field(init=False, default=TRANSFORMED_TARGET)\n    test_fraction: float = field(init=False, default=0.2)\n    seed: int = field(init=False, default=SEED)\n    train: DataFrame = field(init=False)\n    test: DataFrame = field(init=False)\n    transformed_train: DataFrame = field(init=False)\n    transformed_test: DataFrame = field(init=False)\n    lmda: float = field(init=False)\n    \n    def __post_init__(self):\n        self.train, self.test = self.df.randomSplit(\n            [1.0 - self.test_fraction, self.test_fraction], seed=self.seed\n        )\n        self.transformed_train, self.lmda = yeojohnson_transform_on_train_test(df=self.train)\n        self.transformed_test, _ = yeojohnson_transform_on_train_test(df=self.test, lmda=self.lmda)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"787a2cce-a638-42b2-a907-52806f97e01e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%%add_to Experiment\ndef get_pipeline(self):\n  stages = get_pipe_stages(df=self.df)\n  stages += [self.predictor]\n  return Pipeline(stages=stages)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d3cdd703-06d5-405a-a22e-423959b83daf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%%add_to Experiment\ndef log_mlflow_info(self, train_df: DataFrame, test_df: DataFrame) -> None:\n  \"\"\"The function prints logged information of the mlflow run.\n  :param df: A pyspark.sql.dataframe.DataFrame object.\n  :parma model_name: The model name as identifier. \n  :return: None - No returned value.\n  \"\"\"\n  import logging\n  logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n  \n  mlflow.autolog(log_models=False)\n  train_count, test_count = self.train.count(), self.test.count()\n  mlflow.log_param('train_count', train_count)\n  mlflow.log_param('test_count', test_count)\n  print('\\033[36m' + f'Train set size: {train_count}')\n  print('\\033[36m' + f'Test set size: {test_count}')\n  mlflow.log_param('yeojohnson lambda:', self.lmda)\n  print(f\"\\nyeojohnson lambda: {self.lmda}\\n\", \"=\" * 100)\n  # Calculate metrics\n  train_rmse, train_mean, train_rmse_mean_ratio = calc_metrics(df=train_df, model_name=self.model_name, ds_name='train')\n  test_rmse, test_mean, test_rmse_mean_ratio = calc_metrics(df=test_df, model_name=self.model_name)\n  # Log metrics\n  mlflow.log_metric(\"train_rmse\", train_rmse)\n  mlflow.log_metric(\"train_mean\", train_mean)\n  mlflow.log_metric(\"train_rmse_mean_ratio\", train_rmse_mean_ratio)\n  mlflow.log_metric(\"test_rmse\", test_rmse)\n  mlflow.log_metric(\"test_mean\", test_mean)\n  mlflow.log_metric(\"test_rmse_mean_ratio\", test_rmse_mean_ratio)\n  return None"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a9d8ad24-ce4e-4a15-9707-ec80dc6f8397","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%%add_to Experiment\ndef run_mlflow(self) -> DataFrame:\n  pipeline = self.get_pipeline()\n  mlflow.autolog(log_models=False)\n  with mlflow.start_run() as run:\n    model = pipeline.fit(self.transformed_train)\n    # predict on train set for comparing evaluation metrics between train and test sets.\n    train_pred = model.transform(self.transformed_train) \n    test_pred = model.transform(self.transformed_test)    \n    # Log the best model.\n    mlflow.spark.log_model(spark_model=model, artifact_path=self.model_name)\n    # log mlflow run info\n    self.log_mlflow_info(train_df=train_pred, test_df=test_pred)\n    # inverse yeo-johnson transformation on prediction column\n    # Save the predictions data\n    save_table(df=train_pred, file_path=f's3a://{S3_GOLD_PATH}train_{self.model_name}.parquet')\n    save_table(df=test_pred, file_path=f's3a://{S3_GOLD_PATH}test_{self.model_name}.parquet')\n\n  return None"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2e8daaae-9c38-458c-afb4-97cb972e2082","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6600af0d-83e7-4aa7-a46a-4e77a8c8fb7d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ml","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
