{"cells":[{"cell_type":"code","source":["from dataclasses import dataclass\nfrom itertools import chain\nfrom typing import Union\nfrom pyspark.ml.feature import Bucketizer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8aeb424c-7aa8-48df-a11b-f5fc7aa451a8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./utils"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bde56b80-1a96-4a19-9013-2c096e1c085a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./_setup"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"313ad4e7-fe23-4a43-b4eb-6af26fe3c880","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Set constants\nCOLS_TO_DROP = [\n    \"_c0\",\n    \"CompTotal\",\n    \"CompFreq\",\n    \"SurveyLength\",\n    \"SurveyEase\",\n]\nCOLS_TO_SPLIT = [\n    \"DevType\",\n    \"LanguageHaveWorkedWith\",\n    \"LanguageWantToWorkWith\",\n    \"DatabaseHaveWorkedWith\",\n    \"DatabaseWantToWorkWith\",\n    \"PlatformHaveWorkedWith\",\n    \"PlatformWantToWorkWith\",\n    \"WebframeHaveWorkedWith\",\n    \"WebframeWantToWorkWith\",\n    \"MiscTechHaveWorkedWith\",\n    \"MiscTechWantToWorkWith\",\n    \"ToolsTechHaveWorkedWith\",\n    \"ToolsTechWantToWorkWith\",\n    \"NEWCollabToolsHaveWorkedWith\",\n    \"NEWCollabToolsWantToWorkWith\",\n    \"NEWStuck\",\n]\nFINAL_DROP = list(chain(COLS_TO_DROP, COLS_TO_SPLIT))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a78acf96-e282-4a5c-9489-7c0510f67abc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[32]: {'EdLevel': {'Primary/elementary school': 1.0,\n  'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)': 2.0,\n  'Associate degree (A.A., A.S., etc.)': 3.0,\n  'Some college/university study without earning a degree': 4.0,\n  'Something else, Professional degree (JD, MD, etc.)': 5.0,\n  'Bachelor’s degree (B.A., B.S., B.Eng., etc.)': 6.0,\n  'Master’s degree (M.A., M.S., M.Eng., MBA, etc.)': 7.0,\n  'Other doctoral degree (Ph.D., Ed.D., etc.)': 8.0},\n 'Age1stCode': {'Younger than 5 years': 1.0,\n  '5 - 10 years': 2.0,\n  '11 - 17 years': 3.0,\n  '18 - 24 years': 4.0,\n  '25 - 34 years': 5.0,\n  '35 - 44 years': 6.0,\n  '45 - 54 years': 7.0,\n  '55 - 64 years': 8.0,\n  'Older than 64 years': 9.0},\n 'OrgSize': {'Just me - I am a freelancer, sole proprietor, etc.': 1.0,\n  '2 to 9 employees': 2.0,\n  '10 to 19 employees': 3.0,\n  '20 to 99 employees': 4.0,\n  '100 to 499 employees': 5.0,\n  'I don’t know': 6.0,\n  '500 to 999 employees': 7.0,\n  '1,000 to 4,999 employees': 8.0,\n  '5,000 to 9,999 employees': 9.0,\n  '10,000 or more employees': 10.0},\n 'Age': {'Under 18 years old': 1.0,\n  '18-24 years old': 2.0,\n  '25-34 years old': 3.0,\n  '35-44 years old': 4.0,\n  '45-54 years old': 5.0,\n  '55-64 years old': 6.0,\n  '65 years or older': 7.0,\n  'Prefer not to say': 8.0}}"]}],"execution_count":0},{"cell_type":"code","source":["def group_low_freq(\n    *, df: DataFrame, col_name: str, threshold: float = 1.0, group_name: str = \"Other\"\n) -> DataFrame:\n    \"\"\"\n    Group all observations that occur in less then a threshold % of the rows in df column.\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :param col_name: The relevant column to run the function on.\n    :param threshold: Values below the threshold will be set as group_name or 'Other' category.\n    :param group_name: String to use as replacement for the observations that need to be grouped.\n    :return df: A pyspark.sql.dataframe.DataFrame object.\n    \"\"\"\n    col_df = count_by_col(df=df, col_name=col_name)\n    low_freq = col_df.filter(col_df[\"Percentage (%)\"] < threshold)\n    others = list(map(lambda row: row[0], low_freq.select(col_name).collect()))\n    df_with_other = df.withColumn(\n        col_name, f.when(df[col_name].isin(others), \"Other\").otherwise(df[col_name])\n    )\n\n    return df_with_other"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"638b18a5-b15b-4def-9991-eb0733575b41","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def group_low_freq_cols(\n    *, df: DataFrame, cols_to_group: List[str] = [\"Country\", \"Currency\", \"Ethnicity\"]\n) -> DataFrame:\n    \"\"\"The function gets a list of columns as an input and set categories with low freaquency to 'Other' category.\n    :param df: A pyspark.sql.dataframe.DataFrame object.\n    :param cols_to_group: A list of columns names.\n    :return df: A pyspark.sql.dataframe.DataFrame object.\n    \"\"\"\n    for col_name in cols_to_group:\n        df = group_low_freq(df=df, col_name=col_name)\n\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"46d3a397-b630-480f-bd21-643699c24651","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def convert_years_cols(\n    *, df: DataFrame, years_cols: List[str] = [\"YearsCode\", \"YearsCodePro\"]\n) -> DataFrame:\n    \"\"\"The function converts the years column from string type to int type.\n    :param df: A pyspark.sql.dataframe.DataFrame object.\n    :param years_cols: A list of columns names.\n    :return df: A pyspark.sql.dataframe.DataFrame object.\n    \"\"\"\n    for col_name in years_cols:\n        df = df.withColumn(\n            col_name,\n            f.when(f.col(col_name) == \"Less than 1 year\", 0)\n            .when(f.col(col_name) == \"More than 50 years\", 51)\n            .when(f.col(col_name) == \"NA\", 999)\n            .otherwise(f.col(col_name))\n            .cast(\"integer\"),\n        )\n\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"38c6102d-0d05-45f6-9361-9313d8848ea5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def set_bins(\n    *, df: DataFrame, splits: List[Union[int, float]], inputCol: str\n) -> DataFrame:\n    \"\"\"\n    Sets bins for df column values.\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :param splits: A list of bins to split by.\n    :param inputCol: The column name that needs to be splitted to bins.\n    :param outputCol: The name of the output column.\n    :return df: A pyspark.sql.dataframe.DataFrame object.\n    \"\"\"\n    bucketizer = Bucketizer(\n        splits=splits, inputCol=inputCol, outputCol=f\"{inputCol}_rank\"\n    )\n    buck_df = bucketizer.transform(df)\n    # Format bins categories\n    format_udf = udf(\n        lambda x: f\"{splits[int(x)]} - {(splits[int(x) + 1])}\", t.StringType()\n    )\n    bins_df = (\n        buck_df.withColumn(f\"{inputCol}_bin\", format_udf(f\"{inputCol}_rank\"))\n        .drop(inputCol)\n        .drop(f\"{inputCol}_bin\")\n    )\n\n    return bins_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9fea7e04-1c39-485e-aa38-dfa75e2297ea","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def set_bins_for_years_cols(\n    *,\n    df: DataFrame,\n    years_bins: List[int] = [0, 3, 5, 10, 20, 30, 40, 50, 60, 1000],\n    cols_to_bins: List[str] = [\"YearsCode\", \"YearsCodePro\"]\n) -> DataFrame:\n    \"\"\"\n    Sets bins for years columns values of the DataFrame.\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :param years_bins: A list of bins to split by.\n    :param cols_to_bins: The columns names that needs to be splitted to bins.\n    :return df: A pyspark.sql.dataframe.DataFrame\n    \"\"\"\n    for col_name in cols_to_bins:\n        df = set_bins(df=df, splits=years_bins, inputCol=col_name)\n\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"af34264a-90ad-4626-a5ad-b88718b243e4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def replace_to_null(\n    *,\n    df: DataFrame,\n    cols: List[str] = [\"YearsCode_rank\", \"YearsCodePro_rank\"],\n    value: Union[str, int] = 8\n) -> DataFrame:\n    \"\"\"The function gets a DataFrame, col name, and a value and returns a modified DataFrame with None values isntead of the input value.\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :param cols: A list of columns names.\n    :return df: A pyspark.sql.dataframe.DataFrame object.\n    \"\"\"\n    for col_name in cols:\n        df = df.withColumn(\n            col_name,\n            f.when(f.col(col_name) != value, f.col(col_name)).otherwise(f.lit(None)),\n        )\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"14e52d97-ae3a-4389-af93-f29df5877db5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def map_an_ordinal_col(\n    *, df: DataFrame, col_name: str, mapping: Dict[str, int]\n) -> DataFrame:\n    mapping_expr = f.create_map([f.lit(x) for x in chain(*mapping.items())])\n    df = df.withColumn(f\"{col_name}_rank\", mapping_expr.getItem(f.col(col_name))).drop(\n        col_name\n    )\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"35591d66-b7ac-4f5e-bcba-a703a64afb38","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def map_ordinal_cols(\n    *, df: DataFrame, mapping_dict: Dict[str, Dict[str, int]]\n) -> DataFrame:\n    for col_name, mapping in mapping_dict.items():\n        df = map_an_ordinal_col(df=df, col_name=col_name, mapping=mapping)\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0df263b0-2c0e-47fd-94c5-03754cb41245","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# The function below splits a category type column to many columns of the categories with the values of 1 or 0 in each.\n# So why not just using built in OneHotEncoder method of sklearn.preprocessing module?\n# Because the categorical columns in this dataset incloudes thousends of combinations of categorical variables with A LOT of low-represented class Using OneHotEncoder directly, will create a lot of variables.\ndef split_a_column(\n    *, df: DataFrame, col_name: str, delimiter: str = \";\", prefix: str = \"_\"\n) -> DataFrame:\n    \"\"\"The function splits a category type column to many columns of the categories with the values of 1 or 0 in each.\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :param col_name: The column that needs to be splitted.\n    :param delimiter: The delimiter sign. Ex: '|', ';'.\n    :param prefix: The new splitted columns prefix. Set to default of _ as prefix.\n    :return df: A pyspark.sql.dataframe.DataFrame object.\n    \"\"\"\n    delim_df = df.withColumn(f\"{col_name}Arr\", f.split(col_name, delimiter))\n    all_items = delim_df.select(f\"{col_name}Arr\").rdd.flatMap(lambda x: x).collect()\n    flat_list = [\n        item for sub_list in all_items for item in sub_list if str(item) != \"NA\"\n    ]\n    unique_items = set(flat_list)\n    for item in unique_items:\n        item = item.replace(\".\", \" \")\n        new_col_name = f\"{col_name}{prefix}{item}\"\n        df = df.withColumn(\n            f\"{new_col_name}\", f.when(f.col(col_name).like(f\"%{item}%\"), 1).otherwise(0)\n        )\n\n    return df.drop(col_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"48cbeb2a-65e2-4344-842e-0c3a2df802ce","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def split_multiple_columns(*, df: DataFrame, cols: List[str]) -> DataFrame:\n    \"\"\"The function gets a list of columns names and runs the function split_column on the columns in a loop.\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :param cols: A list of columns to run the function on.\n    :param prefix: The new splitted columns prefix. Set to default of no prefix.\n    :return df: A pyspark.sql.dataframe.DataFrame object.\n    \"\"\"\n    for count, col_name in enumerate(cols):\n        df = split_a_column(df=df, col_name=col_name)\n\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fb88540a-784f-4b8c-b2d8-5c998f2e519c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def preprocess_pipeline(*, df: DataFrame) -> DataFrame:\n    \"\"\"\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :return step_7: A pyspark.sql.dataframe.DataFrame object.\n    \"\"\"\n    step_1 = group_low_freq_cols(df=df)\n    step_2 = floor_and_cap_outliers(df=step_1)\n    step_3 = convert_years_cols(df=step_2)\n    step_4 = set_bins_for_years_cols(df=step_3)\n    step_5 = replace_to_null(df=step_4)\n    step_6 = map_ordinal_cols(df=step_5, mapping_dict=ORDINAL_MAPPING)\n    print(f\"Split the following columns: \\n {COLS_TO_SPLIT}\")\n    step_7 = split_multiple_columns(df=step_6, cols=COLS_TO_SPLIT)\n    step_8 = step_7.drop(*FINAL_DROP)\n    step_9 = save_table(\n        df=step_8, file_path=f\"s3a://{S3_PROCESS_PATH}processed_survey.parquet\"\n    )\n    return None"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"64129c9b-7161-486a-95c9-04fb50b1f285","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fd4c921e-1bd8-4f8c-b54c-235ba9428f4c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"preprocess","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
