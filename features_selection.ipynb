{"cells":[{"cell_type":"code","source":["%run ./utils"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"10eaf2a6-3fc0-4c61-b9d6-51cd18ffa1d7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nfrom dython import nominal\n\nfrom pprint import pformat\n\nfrom sklearn.feature_selection import f_regression\nfrom scipy import stats\nfrom phik import phik_from_array"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cd249976-5b72-4a20-a2cf-6b9e5e099bbb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\nFile \u001B[0;32m<command-1220897062489237>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nominal\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# from typing import List\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpprint\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pformat\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n\n\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'dython'","errorSummary":"<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'dython'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n","File \u001B[0;32m<command-1220897062489237>:3\u001B[0m\n","\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n","\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n","\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nominal\n","\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# from typing import List\u001B[39;00m\n","\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpprint\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pformat\n","\n","File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n","\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n","\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n","\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n","\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n","\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n","\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n","\n","\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'dython'"]}}],"execution_count":0},{"cell_type":"code","source":["np.__version__"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d685b553-85c1-483e-8cae-244506218aac","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["TARGET_COL = \"ConvertedCompYearly\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3743ddf7-7714-4167-af15-967a6de4b1b5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["def filter_cols_with_one_value(*, df: DataFrame) -> List[str]:\n    print(\"==== Drop columns with only one value ====\\n\")\n    # apply countDistinct on each column\n    col_counts = (\n        df.agg(*(f.countDistinct(f.col(c)).alias(c) for c in df.columns))\n        .collect()[0]\n        .asDict()\n    )\n    # keep the cols with count > 1\n    cols_to_keep = [c for c in df.columns if col_counts[c] > 1]\n    cols_to_drop = [c for c in df.columns if col_counts[c] == 1]\n    print(\n        f\"=== Number of features in the dataset: {len(df.columns) -1} .   Number of remaining features: {len(cols_to_keep) -1} ===\\n\"\n    )\n    msg = pformat(cols_to_drop)\n    print(f\"Columns to drop:\\n{msg}\\n\")\n    filtered_df = df.drop(*cols_to_drop)\n    return filtered_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"69fe5c16-31f9-499c-8f68-810303cfcff8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# Since the target is not normally distributed we will use the mannwhitneyu test of p-value for selecting significant binary features\ndef filter_binary_cols(*, pdf: pd.DataFrame) -> List[str]:\n    print(\"==== Mannwhitneyu U test for binary features to Target ====\\n\")\n    results = []\n    for column in pdf.columns.to_list():\n        if column == TARGET_COL:\n            continue\n        yes = pdf.loc[pdf[column] == 1, TARGET_COL]\n        no = pdf.loc[pdf[column] == 0, TARGET_COL]\n        statisic, p_val = stats.mannwhitneyu(yes, no)\n        results.append((column, p_val))\n\n    u_df = pd.DataFrame(results, columns=[\"feature\", \"p-value\"])\n    binary_stored_features = u_df[u_df[\"p-value\"] < 0.05].sort_values(by=\"p-value\")\n    print(binary_stored_features)\n    cols_to_keep = binary_stored_features[\"feature\"].to_list()\n    cols_to_drop = [c for c in pdf.columns.to_list() if c not in cols_to_keep]\n    print(\n        f\"\\n=== Number of features in the binary dataset: {len(pdf.columns.to_list()) -1} .   Number of remaining features: {len(cols_to_keep)} ===\\n\"\n    )\n    msg = pformat(cols_to_drop)\n    print(f\"Binary columns to drop: \\n{msg}\\n\")\n    keep_bin_pdf = pdf[cols_to_keep + [TARGET_COL]]\n    return keep_bin_pdf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e1e5e021-e7b7-400a-81c1-77f784fbbaeb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# Pearson correlation for continues features\ndef filter_cols_corr_with_target(\n    *,\n    pdf: pd.DataFrame,\n    min_threshold: float = 0.1,\n    max_threshold: float = 0.9,\n    typ: str = \"numeric\",\n) -> List[str]:\n    print(\"==== Pearson correlation to Target ====\\n\")\n    if typ == \"categorical\":\n        corr_to_target_series = pdf[TARGET_COL].abs().sort_values(ascending=False)\n    else:\n        corr_to_target_series = (\n            pdf.corrwith(pdf[TARGET_COL]).abs().sort_values(ascending=False)\n        )\n    cols_to_keep = corr_to_target_series[\n        (corr_to_target_series > min_threshold)\n        & (corr_to_target_series < max_threshold)\n    ].index.to_list()\n    cols_to_drop = [c for c in pdf.columns.to_list() if c not in cols_to_keep]\n    print(\n        f\"=== Number of features in the {typ} dataset: {len(pdf.columns.to_list()) -1} .   Number of remaining features: {len(cols_to_keep) -1} ===\\n\"\n    )\n    msg = pformat(cols_to_drop)\n    print(f\"Columns to drop:\\n{msg}\\n\")\n    return cols_to_keep"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c1725d52-5c60-4f5b-970d-6f5ec3d2b697","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["def filter_multicullinearity_cols(\n    *, pdf: DataFrame, multi_threshold: float = 0.5, typ: str = \"numeric\"\n) -> List[str]:\n    print(\"==== Multicullinearity with other features ====\\n\")\n    if typ == \"categorical\":\n        corr_matrix = pdf.abs()\n    else:\n        corr_matrix = pdf.corr().abs()\n    # Select upper triangle of correlation matrix\n    upper_triangle = corr_matrix.where(\n        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n    )\n    cols_to_drop = [\n        column\n        for column in upper_triangle.columns\n        if any(upper_triangle[column] > multi_threshold)\n    ]\n    cols_to_keep = [c for c in pdf.columns.to_list() if c not in cols_to_drop]\n    print(\n        f\"=== Number of features in the {typ} dataset: {len(pdf.columns.to_list()) -1} .   Number of remaining features: {len(cols_to_keep) -1} ===\\n\"\n    )\n    msg = pformat(cols_to_drop)\n    print(\n        f\"Features to drop because of multicullinearity with other features: \\n{msg}\\n\"\n    )\n    return cols_to_keep"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6b74b3e6-0174-4531-baee-472be9a1a3b4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["def select_num_features(\n    *,\n    pdf: pd.DataFrame,\n    min_threshold: float = 0.1,\n    max_threshold: float = 0.9,\n    multi_threshold: float = 0.5,\n    typ: str = \"numeric\",\n):\n    print(f\"==== Selecting {typ} featuers ====\\n\")\n    cols_to_keep = filter_cols_corr_with_target(\n        pdf=pdf, min_threshold=min_threshold, max_threshold=max_threshold, typ=typ\n    )\n    keep_pdf = pdf[cols_to_keep]\n    selected_cols = filter_multicullinearity_cols(\n        pdf=keep_pdf, multi_threshold=multi_threshold, typ=typ\n    )\n    return selected_cols"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8739dd2d-a0ba-4b51-b428-f64135237a83","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["def setup_cat_corr(*, pdf: pd.DataFrame) -> pd.DataFrame:\n    assoc = nominal.associations(dataset=pdf, plot=False)\n    corr_pdf = assoc[\"corr\"].copy()\n    cols_names = [c.split(\" (\")[0] for c in corr_pdf.columns.to_list()]\n    corr_pdf.columns = cols_names\n    corr_pdf.index = cols_names\n    return corr_pdf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"61653499-70e3-4718-8957-7d4b3abf873b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["def select_cat_features(\n    *,\n    pdf: pd.DataFrame,\n    min_threshold: float = 0.1,\n    max_threshold: float = 0.9,\n    multi_threshold: float = 0.5\n):\n    print(\"==== Selecting categorical featuers ====\\n\")\n    corr_pdf = setup_cat_corr(pdf=pdf)\n    cols_to_keep = filter_cols_corr_with_target(\n        pdf=corr_pdf,\n        min_threshold=min_threshold,\n        max_threshold=max_threshold,\n        typ=\"categorical\",\n    )\n    keep_pdf = pdf[cols_to_keep]\n    keep_corr_pdf = setup_cat_corr(pdf=keep_pdf)\n    selected_cols = filter_multicullinearity_cols(\n        pdf=keep_corr_pdf, multi_threshold=multi_threshold, typ=\"categorical\"\n    )\n    return selected_cols"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a1a1c49a-9f02-4396-9c84-9142e5034d39","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["def select_features(\n    *,\n    df: DataFrame,\n    id_col: str = \"ResponseId\",\n    min_threshold: float = 0.1,\n    max_threshold: float = 0.9,\n    multi_threshold: float = 0.5,\n) -> DataFrame:\n    \"\"\"The main function for running features selection on all columns data types\"\"\"\n    filtered_df = filter_cols_with_one_value(df=df.drop(id_col))\n    cols_types: Dict = get_cols_by_dtypes(df=filtered_df)\n    bin_pdf = filtered_df.select(*cols_types[\"bin_cols\"]).toPandas()\n    num_pdf = filtered_df.select(*cols_types[\"num_cols\"], TARGET_COL).toPandas()\n    cat_pdf = filtered_df.select(*cols_types[\"cat_cols\"], TARGET_COL).toPandas()\n\n    keep_bin_pdf = filter_binary_cols(pdf=bin_pdf)\n    selected_bin_features = select_num_features(pdf=keep_bin_pdf, typ=\"binary\")\n    selected_num_features = select_num_features(pdf=num_pdf)\n    selected_cat_features = select_cat_features(pdf=cat_pdf)\n    selected_cols = list(\n        chain(\n            [id_col],\n            selected_cat_features,\n            selected_num_features,\n            selected_bin_features,\n            [TARGET_COL],\n        )\n    )\n    print(\"==== Selecting all featuers ====\\n\")\n    print(\n        f\"=== Number of features in the original dataset: {len(df.columns) -1} .   Number of remaining features: {len(selected_cols) -1} ===\\n\"\n    )\n    msg = pformat(selected_cols)\n    print(f\"The final selected features are: \\n{msg}\")\n#     selected_features = [{\"feature\": column} for column in selected_cols]\n#     selected_df = spark.createDataFrame(selected_features)\n    selected_df = df.select(*selected_cols)\n    save_table(\n        df=selected_df, file_path=f\"s3a://{S3_PROCESS_PATH}selected_features.parquet\"\n    )\n    return None"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"154647c6-4730-4d7b-ba19-aac7c36769c0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1a6e1117-be0e-47a9-a841-c12bb0d791f7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"features_selection","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
