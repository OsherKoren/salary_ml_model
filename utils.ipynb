{"cells":[{"cell_type":"code","source":["%run ./config"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4341df72-a62e-4784-aa9d-bff7e0366b4f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from py4j.protocol import Py4JJavaError\nimport re\nfrom typing import Dict, List, Tuple\nfrom pathlib import Path\nfrom pyspark.sql import DataFrame, Window\nfrom pyspark.sql import functions as f\nimport pyspark.sql.types as t\n\n# f_oneway() function takes the group data as input and returns F-statistic and P-value\nfrom scipy.stats import f_oneway"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"033b9230-fa6e-4b27-9aaa-ef1c57c35ad2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create a table from the DataFrame.\n# This table will persist across cluster restarts as well as allow different notebooks to query this data.\ndef save_table(*, df: DataFrame, file_path: str) -> None:\n    \"\"\"The function save a delta table in the process file path\n    :param df: A pyspark.sql.dataframe.DataFrame object to run the function on\n    :param file_path: The file path.\n    :return: None - No returned value.\n    \"\"\"\n    table_name = Path(file_path).stem\n    file_type = Path(file_path).suffix.split(\".\")[-1]\n    # Remove table if it exists\n    import boto3\n\n    s3 = boto3.resource(\n        \"s3\", aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY\n    )\n    bucket = s3.Bucket(S3_BUCKET)\n    prefix = file_path.split(f\"/{S3_BUCKET}\")[-1].strip(\"/\")\n    bucket.objects.filter(Prefix=prefix).delete()\n    # Save table\n    try:\n        df.write.format(file_type).option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(file_path)\n        print(f\"The table: {table_name} was saved.   File path: {file_path}\")\n    except Py4JJavaError as err:\n        print(f\"Py4JJavaError. Could not save the table: {table_name}\\n{err}\")\n    return None"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b56d4874-ef1c-4033-96c9-60510a68a534","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def count_by_col(*, df: DataFrame, col_name: str) -> DataFrame:\n    \"\"\"The function returns a DataFrame that counts the label_col, and calculate the frequency in percentage and the comulative percentage.\n    :param df: A pyspark.sql.dataframe.DataFrame object to run the function on\n    :param label_col: The column to run calaculation on\n    :return: pyspark.sql.dataframe.DataFrame object of the column and it's calculation.\n    \"\"\"\n    total = df.count()\n    counted_df = (\n        df.groupBy(col_name)\n        .count()\n        .withColumn(\"Percentage (%)\", (f.col(\"count\") / total * 100))\n        .withColumn(\"Percentage (%)\", (f.round(f.col(\"Percentage (%)\"), 2)))\n        .withColumn(\n            \"Comulative pct (%)\",\n            f.sum(\"Percentage (%)\").over(\n                Window.orderBy(f.col(\"Percentage (%)\").desc())\n            ),\n        )\n        .withColumn(\"Comulative pct (%)\", (f.round(f.col(\"Comulative pct (%)\"), 2)))\n        .orderBy(\"count\", ascending=False)\n    )\n    return counted_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4a4603cf-4b7d-46b8-89c1-9806bb5b64d4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Performing the ANOVA test\ndef run_anova_test(\n    *, df: DataFrame, col_name: str, target_col: str = \"ConvertedCompYearly\"\n) -> List[int]:\n    \"\"\"The function runs one-way anova tests between a feature column and the target column.\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :param col_name: The relevant column to run the function on.\n    :param target col: The target column.\n    :return: None - No returned value.\n    \"\"\"\n    # Finds out the target data for each category as a list\n    categories_vals = df.toPandas().groupby(col_name)[target_col].apply(list)\n    # We accept the Assumption(H0) only when P-Value > 0.05\n    anova_results = f_oneway(*categories_vals)\n    p_value = anova_results[1]\n    print(\"\\x1b[0;31;47m\" + f\"P-Value for Anova is: {p_value:.4f}\")\n    if p_value > 0.05:\n        print(\n            \"\\x1b[0;31;47m\"\n            + f\"**** We reject the Assumption(H0).  {col_name} and {target_col} are correlated ****\\n\"\n            + \"=\" * 100\n        )\n    else:\n        print(\n            \"\\x1b[0;32;47m\"\n            + f\"**** We reject the Assumption(H0).  {col_name} and {target_col} are correlated ****\\n\"\n            + \"=\" * 100\n        )\n    return None"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3df7330f-0111-4a05-a31b-7602c44f1cdc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def run_multiple_anova_tests(\n    *, df: DataFrame, cols: List[str] = [\"Country\", \"Currency\", \"Ethnicity\"]\n) -> DataFrame:\n    \"\"\"The function gets a list of columns as an input and set categories with low freaquency to 'Other' category.\n    :param df: A pyspark.sql.dataframe.DataFrame object.\n    :param cols: A list of columns names.\n    :return: None - No returned value.\n    \"\"\"\n    for col_name in cols:\n        run_anova_test(df=df, col_name=col_name)\n\n    return None"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c9f34bbc-accd-43b2-895d-b5bb6700beae","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def floor_and_cap_outliers(\n    *, df: DataFrame, quant: float = 0.1, target_col: str = \"ConvertedCompYearly\"\n):\n    \"\"\"Quantile based flooring and capping the outliers\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :param quant: The floor quantile.\n    :param target_col: The target column.\n    :return: None - No returned value.\n    \"\"\"\n    floor_quantile = df.approxQuantile(target_col, [quant], 0)[0]\n    cap_quantile = df.approxQuantile(target_col, [1.0 - quant], 0)[0]\n    df = df.withColumn(\n        target_col,\n        f.when(f.col(target_col) < floor_quantile, floor_quantile)\n        .otherwise(f.col(target_col))\n        .cast(t.DoubleType()),\n    )\n    df = df.withColumn(\n        target_col,\n        f.when(f.col(target_col) > cap_quantile, cap_quantile)\n        .otherwise(f.col(target_col))\n        .cast(t.DoubleType()),\n    )\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1bc45487-61c1-4947-a365-e3063959bf86","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def extract_cols_names_by_regex(\n    *, df: DataFrame, pattern: str = \"(^Total|_rank$)\"\n) -> List[str]:\n    \"\"\"The function extract columns name with the relevant substring\n    :param df: A pyspark.sql.dataframe.DataFrame object to run the function on\n    :param pattern: A regex to extract by.\n    :return: A list of origin columns names substrings\n    \"\"\"\n    cols_names = []\n    for col_name in df.columns:\n        match = re.search(pattern, col_name)\n        if match:\n            cols_names.append(col_name)\n\n    return list(set(cols_names))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3665c954-123e-49f3-9194-d1d4689b56ed","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def sum_cols_group_by_prefix(*, df: DataFrame, col_prefix: str) -> DataFrame:\n    \"\"\"The function add a sum column of a group of columns by the same prefix\n    :param df: A pyspark.sql.dataframe.DataFrame object to run the function on\n    :param col_prefix: The prefix to search for\n    :return: A pyspark.sql.dataframe.DataFrame with an additional Total column that sum the values of the group columns.\n    \"\"\"\n    cols_group = [\n        col_name for col_name in df.columns if col_name.startswith(col_prefix)\n    ]\n    return df.withColumn(f\"Total_{col_prefix}\", sum(df[c] for c in cols_group))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4e5d973e-ac3a-4c30-8721-0ee4d7bfc400","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def replace_na_cols_to_null(*, df: DataFrame) -> DataFrame:\n    \"\"\"The function gets a DataFrame, col name, and a value and returns a modified DataFrame with None values isntead of the input value.\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :param cols: A list of columns names.\n    :return df: A pyspark.sql.dataframe.DataFrame object.\n    \"\"\"\n    for col_name in df.columns:\n        df = df.withColumn(\n            col_name,\n            f.when(f.col(col_name) != \"NA\", f.col(col_name)).otherwise(f.lit(None)),\n        )\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e24fecc1-d567-48eb-9374-a64c6ef8daa2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_cols_by_dtypes(*, df: DataFrame) -> Dict[str, List[str]]:\n    \"\"\"The function gets a DataFrame and returns a dictionary of columns dtypes.\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :return: A dictionary with the column dtype as the key, and a list of relevant columns names as the value.\n    \"\"\"\n    str_cols = [column[0] for column in df.dtypes if column[1] == \"string\"]\n    numeric_cols = [column[0] for column in df.dtypes if column[1] in (\"double\", \"int\")]\n    # Excluding splitted columns from numeric_cols list because they are binary with values of 0 and 1\n    num_cols = extract_cols_names_by_regex(df=df)\n    bin_cols = [col_name for col_name in numeric_cols if col_name not in num_cols]\n    return {\n        \"cat_cols\": str_cols,\n        \"bin_cols\": bin_cols,\n        \"num_cols\": num_cols,\n    }"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"932537cf-1deb-4df6-bc35-bcb7e53744f1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# def isinf(*, f_col: Column) -> Column:\n#   \"\"\"Check if a column contains infinity values.\n#   param f_col (pyspark.sql.column.Column): The column to check for infinity values.\n#   Returns: pyspark.sql.column.Column: A boolean column indicating whether each value in `f_col` is infinity.\n#   \"\"\"\n#   return (f.col == float(\"inf\")) | (f.col == float(\"-inf\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7e194971-91c2-4d8c-8dcd-1c73d2fd1b4a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_cols_with_null_values(*, df: DataFrame) -> List[str]:\n    null_columns = []\n    for column in df.columns:\n        if df.filter(f.col(column).isNull() | f.isnan(column)).filter(f.col(column).isNull()).count() > 0:\n            null_columns.append(column)\n    return null_columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"80e5ede6-bbfe-44a1-b816-0177986b524c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def count_nulls(*, df: DataFrame) -> Dict[str, int]:\n    \"\"\"\n    Counts the number of null values in each column of a Spark DataFrame.\n    :param dataframe: Spark DataFrame to count null values for\n    :return: Dictionary of column names and corresponding null value counts\n    \"\"\"\n    null_counts = {}\n    for column in df.columns:\n        count = df.filter(f.col(column).isNull()).count()\n        null_counts[column] = count\n    return null_counts"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"34895966-55d5-44a5-9a67-d3d5e2d3f481","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"utils","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
