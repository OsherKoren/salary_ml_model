{"cells":[{"cell_type":"code","source":["%run ./utils"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"48046fb6-3588-45f1-bff0-33fae211c443","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./_setup"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"29e37647-59f4-4c6e-a308-0cb657be0a20","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import (\n    OneHotEncoder,\n    StringIndexer,\n    VectorAssembler,\n)\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\nimport jdc\nfrom itertools import chain\nfrom typing import Any, Dict, List, Literal, Optional, Tuple, Union\nimport numpy as np\nfrom numpy.typing import ArrayLike\nfrom sparkdl.xgboost import XgboostRegressor\nfrom scipy import stats, special"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ea44f9dc-7ee9-46ef-988c-1802685e1b80","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stderr","text":["/databricks/python_shell/dbruntime/PostImportHook.py:184: FutureWarning: `sparkdl.xgboost` is deprecated and will be removed in a future Databricks Runtime release. Use `xgboost.spark` instead. See https://docs.databricks.com/machine-learning/train-model/xgboost-spark.html#xgboost-migration for migration.\n  hook(module)\n"]}],"execution_count":0},{"cell_type":"code","source":["ModelRegressor = Union[XgboostRegressor, RandomForestRegressor]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"af56b77f-222f-4c5a-9c1f-7dead2123693","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[31]: {'EdLevel': {'Primary/elementary school': 1.0,\n  'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)': 2.0,\n  'Associate degree (A.A., A.S., etc.)': 3.0,\n  'Some college/university study without earning a degree': 4.0,\n  'Something else, Professional degree (JD, MD, etc.)': 5.0,\n  'Bachelor’s degree (B.A., B.S., B.Eng., etc.)': 6.0,\n  'Master’s degree (M.A., M.S., M.Eng., MBA, etc.)': 7.0,\n  'Other doctoral degree (Ph.D., Ed.D., etc.)': 8.0},\n 'Age1stCode': {'Younger than 5 years': 1.0,\n  '5 - 10 years': 2.0,\n  '11 - 17 years': 3.0,\n  '18 - 24 years': 4.0,\n  '25 - 34 years': 5.0,\n  '35 - 44 years': 6.0,\n  '45 - 54 years': 7.0,\n  '55 - 64 years': 8.0,\n  'Older than 64 years': 9.0},\n 'OrgSize': {'Just me - I am a freelancer, sole proprietor, etc.': 1.0,\n  '2 to 9 employees': 2.0,\n  '10 to 19 employees': 3.0,\n  '20 to 99 employees': 4.0,\n  '100 to 499 employees': 5.0,\n  'I don’t know': 6.0,\n  '500 to 999 employees': 7.0,\n  '1,000 to 4,999 employees': 8.0,\n  '5,000 to 9,999 employees': 9.0,\n  '10,000 or more employees': 10.0},\n 'Age': {'Under 18 years old': 1.0,\n  '18-24 years old': 2.0,\n  '25-34 years old': 3.0,\n  '35-44 years old': 4.0,\n  '45-54 years old': 5.0,\n  '55-64 years old': 6.0,\n  '65 years or older': 7.0,\n  'Prefer not to say': 8.0}}"]}],"execution_count":0},{"cell_type":"code","source":["TARGET_COL = \"ConvertedCompYearly\"\nCOLS_TO_SPLIT = [\n    \"DevType\",\n    \"LanguageHaveWorkedWith\",\n    \"LanguageWantToWorkWith\",\n    \"DatabaseHaveWorkedWith\",\n    \"DatabaseWantToWorkWith\",\n    \"PlatformHaveWorkedWith\",\n    \"PlatformWantToWorkWith\",\n    \"WebframeHaveWorkedWith\",\n    \"WebframeWantToWorkWith\",\n    \"MiscTechHaveWorkedWith\",\n    \"MiscTechWantToWorkWith\",\n    \"ToolsTechHaveWorkedWith\",\n    \"ToolsTechWantToWorkWith\",\n    \"NEWCollabToolsHaveWorkedWith\",\n    \"NEWCollabToolsWantToWorkWith\",\n    \"NEWStuck\",\n]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b6a3fd30-ec1c-4834-af62-e6cf1cac59f8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def sum_splitted_cols(*, df: DataFrame, cols: List[str] = COLS_TO_SPLIT) -> DataFrame:\n    \"\"\"The function runs on the columns that were splitted, and sum the values of each group of columns in a new column.\n    :param df: A pyspark.sql.dataframe.DataFrame object.\n    :return df: A pyspark.sql.dataframe.DataFrame object.\n    \"\"\"\n    for col_name in cols:\n        df = sum_cols_group_by_prefix(df=df, col_prefix=col_name)\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b195b1a5-9b96-486c-b7ad-1971e8f5774f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def _set_df(\n    *, cols_to_drop: Optional[List] = [], selected_features: bool = False\n) -> DataFrame:\n    \"\"\"Set the dataset to run features engineering and to train on.\n    :return: A pyspark.sql.dataframe.DataFrame object with pre-processed and selected data.\n    \"\"\"\n    base_df = spark.read.parquet(\n        f\"s3a://{S3_PROCESS_PATH}enriched_survey.parquet\"\n    ).cache()\n    if selected_features:\n        base_df = spark.read.parquet(\n            f\"s3a://{S3_PROCESS_PATH}selected_features.parquet\"\n        ).cache()\n        \n    df = base_df.drop(*cols_to_drop) if cols_to_drop else base_df\n    return df.select(\n        [\n            f.when(f.col(c) == \"NA\", None).otherwise(f.col(c)).alias(c)\n            for c in df.columns\n        ]\n    )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3fba3919-7791-4f2a-9e36-90c7d26f9c88","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def yeojohnson_transform_train(\n    *, df: DataFrame, col_name: str = TARGET_COL\n) -> Tuple[ArrayLike, float]:\n    col_vals = np.array(df.select(col_name).collect()).reshape(-1)  # for 1-D array\n    transformed, lmda = stats.yeojohnson(col_vals)\n    return transformed.tolist(), lmda"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d13f36cb-150f-460f-a876-223b67cb78fe","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def yeojohnson_transform_test(\n    *, df: DataFrame, lmda: float, col_name: str = TARGET_COL\n) -> ArrayLike:\n    col_vals = np.array(df.select(col_name).collect()).reshape(-1)  # for 1-D array\n    transformed = stats.yeojohnson(col_vals, lmbda=lmda)\n    return transformed.tolist()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ed1dc9cb-dd95-4409-83e2-6689875f661d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def yeojohnson_transform_on_train_test(\n    *,\n    df: DataFrame,\n    lmda: Union[Literal[None], float] = None,\n    col_name: str = TARGET_COL,\n):\n    if lmda is None:\n        transformed_arr, lmda = yeojohnson_transform_train(df=df)\n    else:\n        transformed_arr = yeojohnson_transform_test(df=df, lmda=lmda)\n    # Add the new column to the original DataFrame using a monotonically increasing ID\n    transformed_df = df.repartition(1).withColumn(\n        f\"transformed_{col_name}\",\n        f.udf(lambda i: transformed_arr[i])(f.monotonically_increasing_id()).cast(\n            \"double\"\n        ),\n    )\n    return transformed_df, lmda"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"41a5d00a-ede0-4d77-83b3-070770a55706","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_inversed_yeojohnson_df(\n    *, df: DataFrame, lmda: float, col_name: str = \"prediction\"\n) -> DataFrame: #ArrayLike:\n    col_vals = (\n        np.array(df.select(col_name).collect()).reshape(-1)\n    )  # for 1-D array\n    inversed_vals = special.inv_boxcox(col_vals, lmda)\n    col_vals_list, inversed_vals_list = col_vals.tolist(), inversed_vals.tolist()\n    df = spark.createDataFrame(zip(col_vals_list, inversed_vals_list), schema=[col_name, f\"inversed_{col_name}\"])\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"30a27074-6531-4130-9cc5-e2c1aab80c3e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Apply the inverse Yeo-Johnson transformation to the column\ndef inverse_yeojohnson(\n    *, df: DataFrame, lmda: float, col_name: str = \"prediction\"\n) -> DataFrame:\n    # Add the new column to the original DataFrame using a monotonically increasing ID\n    inversed_df = get_inversed_yeojohnson_df(df=df, lmda=lmda, col_name=col_name)\n    joined_df = df.join(inversed_df, col_name, \"left\")\n    joined_df = joined_df.withColumnRenamed(\n        col_name, f\"transformed_{col_name}\"\n    ).withColumnRenamed(f\"inversed_{col_name}\", col_name)\n    # round the column\n    joined_df = joined_df.withColumn(col_name, f.round(joined_df[col_name], 0))\n    return joined_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"504927aa-b730-468b-810e-85b9d2fd25d8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_indexers(*, cat_cols: List[str]) -> List[StringIndexer]:\n    \"\"\"The function index categorical features.\n    :param cat_cols: A list of categorical columns.\n    :return indexers: List[StringIndexer]\n    \"\"\"\n    indexers = [\n        StringIndexer(inputCol=c, outputCol=f\"{c}Index\", handleInvalid=\"keep\")\n        for c in cat_cols\n    ]\n    return indexers"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9eef4d21-d923-4e9d-ba09-90521be6a979","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_encoders(*, indexers: List[StringIndexer]) -> List[OneHotEncoder]:\n    \"\"\"The function encode categorical features that were indexed.\n    :param indexers: List[StringIndexer]\n    :return encoders: List[OneHotEncoder]\n    \"\"\"\n    encoders = [\n        OneHotEncoder(\n            inputCol=indexer.getOutputCol(),\n            outputCol=f\"{indexer.getOutputCol()}Encoded\",\n        )\n        for indexer in indexers\n    ]\n    return encoders"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"60c6defc-2171-4e6a-9eb0-3ce7ecc199a2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_assembler(\n    *, cols_dict: Dict[str, List[str]], encoders: List[OneHotEncoder]\n) -> VectorAssembler:\n    \"\"\"The function vectorize the features.\n    :param cols_dict: Dict[str, List[str]]\n    :param encoders: List[OneHotEncoder]\n    :return assembler: VectorAssembler\n    \"\"\"\n    cat_cols = [encoder.getOutputCol() for encoder in encoders]\n    bin_cols, num_cols = cols_dict[\"bin_cols\"], cols_dict[\"num_cols\"]\n    featuresCols = list(chain(cat_cols, bin_cols, num_cols))\n    featuresCols.remove(TARGET_COL)\n    assembler = VectorAssembler(\n        inputCols=featuresCols, outputCol=\"features\", handleInvalid=\"keep\"\n    )\n    return assembler"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c5c9fd56-7e17-41ad-b8e8-6b5141f8c311","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_pipe_stages(*, df: DataFrame) -> List[Any]:\n    \"\"\"The function builds stages for transformation and model pipeline\n    :param df: A pyspark.sql.dataframe.DataFrame\n    :return stages: A list of stages of features transformations to be set for the ML pipeline.\n    \"\"\"\n    cols_dict = get_cols_by_dtypes(df=df)\n    indexers = get_indexers(cat_cols=cols_dict[\"cat_cols\"])\n    encoders = get_encoders(indexers=indexers)\n    assembler = get_assembler(cols_dict=cols_dict, encoders=encoders)\n    stages = list(chain(indexers, encoders, [assembler]))\n    return stages"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d46077d9-e9dd-426a-93fc-9ca46b273c11","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def _set_rf_reg_param_grid(\n    *,\n    estimator: RandomForestRegressor,\n    maxDepth: List[int] = [5],\n    maxBins: List[int] = [32],\n    minInstancesPerNode: List[int] = [1],\n    minInfoGain: List[float] = [0.0],\n    checkpointInterval: List[int] = [10],\n    subsamplingRate: List[float] = [1.0],\n    numTrees: List[int] = [20],\n    minWeightFractionPerNode: List[float] = [0.0],\n) -> Dict[str, List[Any]]:\n    \"\"\"Define a grid of hyperparameters to test. All parameters are set to the default parameters of sklearn.xgboost\n      :param estimator: RandomForestRegressor\n      :param maxDepth: The maximum depth of the tree\n      :param gamma: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n      :param learning_rate: Step size shrinkage used in the update to prevent overfitting.\n      :param min_samples_split: The minimum number of samples required to split an internal node.\n      :param n_estimators: The numbers of trees used by the algorithm.\n      :param subsample: Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n      :return paramGrid: Dict[str, List[Any]]\n    #\"\"\"\n    paramGrid = (\n        ParamGridBuilder()\n        .addGrid(estimator.maxDepth, maxDepth)\n        .addGrid(estimator.maxBins, maxBins)\n        .addGrid(estimator.minInstancesPerNode, minInstancesPerNode)\n        .addGrid(estimator.minInfoGain, minInfoGain)\n        .addGrid(estimator.checkpointInterval, checkpointInterval)\n        .addGrid(estimator.subsamplingRate, subsamplingRate)\n        .addGrid(estimator.numTrees, numTrees)\n        .addGrid(estimator.minWeightFractionPerNode, minWeightFractionPerNode)\n        .build()\n    )\n    return paramGrid"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"caa2cbdc-644d-4f0c-994f-dc54daee4f5c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def _set_xgb_reg_param_grid(\n    *,\n    estimator: XgboostRegressor,\n    colsample_bytree: List[float] = [1.0],\n    gamma: List[float] = [0.0],\n    learning_rate: List[float] = [0.3],\n    max_depth: List[int] = [6],\n    min_child_weight=[1],\n    n_estimators: List[int] = [100],\n    subsample: List[float] = [1.0],\n) -> Dict[str, List[Any]]:\n    \"\"\"Define a grid of hyperparameters to test. All parameters are set to the default parameters of sklearn.xgboost\n    :param estimator: XgboostRegressor\n    :param colsample_bytree: The subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n    :param gamma: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n    :param learning_rate: Step size shrinkage used in the update to prevent overfitting.\n    :param max_depth: The depth of each desicsion tree.\n    :param min_child_weight: Minimum sum of instance weight (hessian) needed in a child.\n    :param n_estimators: The numbers of trees used by the algorithm.\n    :param subsample: Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n    :return paramGrid: Dict[str, List[Any]]\n    \"\"\"\n    paramGrid = (\n        ParamGridBuilder()\n        .addGrid(estimator.colsample_bytree, colsample_bytree)\n        .addGrid(estimator.gamma, gamma)\n        .addGrid(estimator.learning_rate, learning_rate)\n        .addGrid(estimator.max_depth, max_depth)\n        .addGrid(estimator.min_child_weight, min_child_weight)\n        .addGrid(estimator.n_estimators, n_estimators)\n        .addGrid(estimator.subsample, subsample)\n        .build()\n    )\n    return paramGrid"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4d44ff38-3711-4f7d-b294-f20f1f81423c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def tune_param(\n    *,\n    estimator: ModelRegressor,\n    metric_name: str = \"rmse\",\n    param_grid: Dict[str, List[Any]],\n    numFolds: int = 5\n) -> CrossValidator:\n    \"\"\"Tune parameters for best result of the defined evaluation metric.\n    The CrossValidator compares the true labels with predicted values for each combination of parameters, and calculates this value to determine the best model.\n    :param estimator: ModelRegressor\n    :param metric_name: The evaluation metric for the best result check\n    :param param_grid: A grid of hyperparameters to test\n    :param numFolds: Number of k-fold for splitting the data\n    :return cv: CrossValidator\n    \"\"\"\n    evaluator = RegressionEvaluator(\n        metricName=metric_name,\n        labelCol=estimator.getLabelCol(),\n        predictionCol=estimator.getPredictionCol(),\n    )\n    # Declare the CrossValidator, which performs the model tuning.\n    cv = CrossValidator(\n        estimator=estimator,\n        evaluator=evaluator,\n        estimatorParamMaps=param_grid,\n        collectSubModels=True,\n    )\n    return cv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f32d0d07-6a6f-4494-8015-7b06b84df84c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"feature_engineering","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
